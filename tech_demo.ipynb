{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDI-1004 Tech Demo\n",
    "\n",
    "Thomas Shank - 200346862\n",
    "\n",
    "DTT - Deep Talk Translator\n",
    "\n",
    "DTT is a audio hot-translator that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 - Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cache Setup\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"./.tech_demo/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from encoding import Translator, Transcriber, Synthesizer\n",
    "from recording import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Record Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio instantiated\n",
      "Press R to start recording audio [ouptut=input.wav]\n",
      "Press S to stop recording...\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Audio Class\n",
    "audio = Audio()\n",
    "\n",
    "# Record (R to start, S to stop)\n",
    "audio.record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Transcribe Audio $\\rightarrow$ Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech-to-text instantiated [device=cuda:0]\n",
      "Text from audio: [ Hello, this is Thomas and thank you for listening to my presentation on DeepTalk Translator.]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Transcriber\n",
    "transcriber = Transcriber()\n",
    "\n",
    "# Transcribe\n",
    "transcription = transcriber.transcribe(audio.filename())\n",
    "\n",
    "print(f\"Text from audio: [{transcription['text']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - Translate Text $\\rightarrow$ Desired Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translator instantiated\n",
      "Translating: [lang=fr, text= Hello, this is Thomas and thank you for listening to my presentation on DeepTalk Translator.]\n",
      "Translated to fr: [Bonjour, je suis Thomas et je vous remercie d'avoir écouté ma présentation sur le traducteur Deeptalk.]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Translator\n",
    "translator = Translator()\n",
    "\n",
    "# Translate\n",
    "lang_code = \"fr\"\n",
    "\n",
    "print(f\"Translating: [lang={lang_code}, text={transcription['text']}]\")\n",
    "translation = translator.translate(transcription[\"text\"], lang_code)\n",
    "\n",
    "print(f\"Translated: [lang={lang_code}, translation={translation}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Synthesize Text $\\rightarrow$ Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech initialized [device=cuda]\n",
      "Synthesizing: [lang=fr, text=Bonjour, je suis Thomas et je vous remercie d'avoir écouté ma présentation sur le traducteur Deeptalk.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\tech_demo.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomsr/Documents/School/aidi/AIDI1004%20-%20Issues%20and%20Challenges%20in%20AI/week-10/tech_demo.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Synthesize the text and write to output.wav\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomsr/Documents/School/aidi/AIDI1004%20-%20Issues%20and%20Challenges%20in%20AI/week-10/tech_demo.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSynthesizing: [lang=\u001b[39m\u001b[39m{\u001b[39;00mlang_code\u001b[39m}\u001b[39;00m\u001b[39m, text=\u001b[39m\u001b[39m{\u001b[39;00mtranslation\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomsr/Documents/School/aidi/AIDI1004%20-%20Issues%20and%20Challenges%20in%20AI/week-10/tech_demo.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m synthesized \u001b[39m=\u001b[39m synthesizer\u001b[39m.\u001b[39;49msynthesize(lang_code, translation)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomsr/Documents/School/aidi/AIDI1004%20-%20Issues%20and%20Challenges%20in%20AI/week-10/tech_demo.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished synthesizing, writing to output.wav\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomsr/Documents/School/aidi/AIDI1004%20-%20Issues%20and%20Challenges%20in%20AI/week-10/tech_demo.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m synthesizer\u001b[39m.\u001b[39mwrite(synthesized)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\encoding\\synthesizer.py:33\u001b[0m, in \u001b[0;36mSynthesizer.synthesize\u001b[1;34m(self, lang, text)\u001b[0m\n\u001b[0;32m     29\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_id, voice_preset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_voices[lang]\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m inputs \u001b[39m=\u001b[39m processor(text\u001b[39m=\u001b[39m[text], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[1;32m---> 33\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, pad_token_id\u001b[39m=\u001b[39;49m\u001b[39m1337\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:1804\u001b[0m, in \u001b[0;36mBarkModel.generate\u001b[1;34m(self, input_ids, history_prompt, return_output_lengths, **kwargs)\u001b[0m\n\u001b[0;32m   1801\u001b[0m             kwargs_fine[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m   1803\u001b[0m \u001b[39m# 1. Generate from the semantic model\u001b[39;00m\n\u001b[1;32m-> 1804\u001b[0m semantic_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msemantic\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m   1805\u001b[0m     input_ids,\n\u001b[0;32m   1806\u001b[0m     history_prompt\u001b[39m=\u001b[39;49mhistory_prompt,\n\u001b[0;32m   1807\u001b[0m     semantic_generation_config\u001b[39m=\u001b[39;49msemantic_generation_config,\n\u001b[0;32m   1808\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_semantic,\n\u001b[0;32m   1809\u001b[0m )\n\u001b[0;32m   1811\u001b[0m \u001b[39m# 2. Generate from the coarse model\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m coarse_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoarse_acoustics\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m   1813\u001b[0m     semantic_output,\n\u001b[0;32m   1814\u001b[0m     history_prompt\u001b[39m=\u001b[39mhistory_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1819\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_coarse,\n\u001b[0;32m   1820\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:992\u001b[0m, in \u001b[0;36mBarkSemanticModel.generate\u001b[1;34m(self, input_ids, semantic_generation_config, history_prompt, attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m    986\u001b[0m early_stopping_logits_processor \u001b[39m=\u001b[39m BarkEosPrioritizerLogitsProcessor(\n\u001b[0;32m    987\u001b[0m     eos_token_id\u001b[39m=\u001b[39msemantic_generation_config\u001b[39m.\u001b[39meos_token_id, min_eos_p\u001b[39m=\u001b[39mmin_eos_p\n\u001b[0;32m    988\u001b[0m )\n\u001b[0;32m    990\u001b[0m \u001b[39m# pass input_ids in order to stay consistent with the transformers generate method even though it is not used\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m# (except to get the input seq_len - that's why we keep the first 257 tokens)\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m semantic_output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m    993\u001b[0m     torch\u001b[39m.\u001b[39;49mones((batch_size, max_input_semantic_length \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice),\n\u001b[0;32m    994\u001b[0m     input_embeds\u001b[39m=\u001b[39;49minput_embeds,\n\u001b[0;32m    995\u001b[0m     logits_processor\u001b[39m=\u001b[39;49m[suppress_tokens_logits_processor, early_stopping_logits_processor],\n\u001b[0;32m    996\u001b[0m     generation_config\u001b[39m=\u001b[39;49msemantic_generation_config,\n\u001b[0;32m    997\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    998\u001b[0m )  \u001b[39m# size: 10048\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[39m# take the generated semantic tokens\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m semantic_output \u001b[39m=\u001b[39m semantic_output[:, max_input_semantic_length \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m :]\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\generation\\utils.py:1800\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1792\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1793\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1794\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1795\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1796\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1797\u001b[0m     )\n\u001b[0;32m   1799\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[0;32m   1801\u001b[0m         input_ids,\n\u001b[0;32m   1802\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[0;32m   1803\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[0;32m   1804\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[0;32m   1805\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[0;32m   1806\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[0;32m   1807\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[0;32m   1808\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[0;32m   1809\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[0;32m   1810\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[0;32m   1811\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m   1812\u001b[0m     )\n\u001b[0;32m   1814\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1815\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1816\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1817\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1818\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1823\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m   1824\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\generation\\utils.py:2897\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2894\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2896\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2897\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[0;32m   2898\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[0;32m   2899\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2900\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   2901\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   2902\u001b[0m )\n\u001b[0;32m   2904\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2905\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\accelerate\\hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:835\u001b[0m, in \u001b[0;36mBarkCausalModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, position_ids, head_mask, labels, input_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    825\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    826\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    827\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m         output_attentions,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 835\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    836\u001b[0m         hidden_states,\n\u001b[0;32m    837\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_layer_key_values,\n\u001b[0;32m    838\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    839\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    840\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    841\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    842\u001b[0m     )\n\u001b[0;32m    844\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:453\u001b[0m, in \u001b[0;36mBarkBlock.forward\u001b[1;34m(self, hidden_states, past_key_values, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    449\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n\u001b[0;32m    451\u001b[0m intermediary_hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m attn_output\n\u001b[0;32m    452\u001b[0m intermediary_hidden_states \u001b[39m=\u001b[39m intermediary_hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\n\u001b[1;32m--> 453\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayernorm_2(intermediary_hidden_states)\n\u001b[0;32m    454\u001b[0m )\n\u001b[0;32m    456\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n\u001b[0;32m    457\u001b[0m     outputs \u001b[39m=\u001b[39m (intermediary_hidden_states,) \u001b[39m+\u001b[39m outputs\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:390\u001b[0m, in \u001b[0;36mBarkLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mshape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, eps\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\tomsr\\Documents\\School\\aidi\\AIDI1004 - Issues and Challenges in AI\\week-10\\.tech_demo\\Lib\\site-packages\\torch\\nn\\functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   2540\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2541\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[0;32m   2542\u001b[0m     )\n\u001b[1;32m-> 2543\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate Synthesizer\n",
    "synthesizer = Synthesizer()\n",
    "\n",
    "# Synthesize the text and write to output.wav\n",
    "print(f\"Synthesizing: [lang={lang_code}, text={translation}]\")\n",
    "synthesized = synthesizer.synthesize(lang_code, translation)\n",
    "\n",
    "print(\"Finished synthesizing, writing to output.wav\")\n",
    "synthesizer.write(synthesized)\n",
    "\n",
    "print(\"Synthesized output written!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tech_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
